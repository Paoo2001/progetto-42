{"cells":[{"cell_type":"markdown","metadata":{"id":"gH04G_mcOZ6C"},"source":["# Modello CNN\n","Genereazione di mappe di perfusione cerebrale rCBV a partire da immagini di Risonanza Magnetica DSC utilizzando un modello CNN."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9721,"status":"ok","timestamp":1684226618382,"user":{"displayName":"Mario Di Giura","userId":"01764325961354702111"},"user_tz":-120},"id":"D3AZ8XAOYUGc","outputId":"80b60d5f-7210-4313-b01c-6e2ddbacabc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n","Collecting torchio\n","  Downloading torchio-0.18.91-py2.py3-none-any.whl (172 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.8/172.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n","Collecting Deprecated (from torchio)\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Collecting SimpleITK!=2.0.*,!=2.1.1.1 (from torchio)\n","  Downloading SimpleITK-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from torchio) (4.6.0)\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from torchio) (3.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torchio) (1.10.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchio) (4.65.0)\n","Requirement already satisfied: typer[all] in /usr/local/lib/python3.10/dist-packages (from torchio) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->torchio) (1.14.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]->torchio) (8.1.3)\n","Collecting colorama<0.5.0,>=0.4.3 (from typer[all]->torchio)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]->torchio)\n","  Downloading shellingham-1.5.0.post1-py2.py3-none-any.whl (9.4 kB)\n","Collecting rich<13.0.0,>=10.11.0 (from typer[all]->torchio)\n","  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0 (from rich<13.0.0,>=10.11.0->typer[all]->torchio)\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0.0,>=10.11.0->typer[all]->torchio) (2.14.0)\n","Installing collected packages: SimpleITK, commonmark, torchinfo, shellingham, rich, Deprecated, colorama, torchmetrics, torchio\n","  Attempting uninstall: rich\n","    Found existing installation: rich 13.3.4\n","    Uninstalling rich-13.3.4:\n","      Successfully uninstalled rich-13.3.4\n","Successfully installed Deprecated-1.2.13 SimpleITK-2.2.1 colorama-0.4.6 commonmark-0.9.1 rich-12.6.0 shellingham-1.5.0.post1 torchinfo-1.8.0 torchio-0.18.91 torchmetrics-0.11.4\n"]}],"source":["!pip install numpy torch torchvision torchio matplotlib torchinfo torchmetrics"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPze4KhFOegC","outputId":"2b393edb-d989-4f3c-a48c-e0e9045cfe5d","executionInfo":{"status":"ok","timestamp":1684226651661,"user_tz":-120,"elapsed":33283,"user":{"displayName":"Mario Di Giura","userId":"01764325961354702111"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# imports\n","import torch\n","import torchio as tio\n","import numpy as np\n","import os\n","from torchinfo import summary\n","import skimage #serve per elaborazione immagini (nel nostro caso usiamo per le metriche)\n","import torch.nn as nn #ci sono moduli per alcuni modelli neurali, regressione, convoluzione ecc\n","import torch.nn.functional as F #qui invece ci possono essere buoni comandi per kernel, filtraggi, max pooling e cose così\n","import shutil\n","import torchvision\n","from torchvision import models, transforms as tt\n","import matplotlib.pyplot as plt\n","import nibabel as nib #libreria per la gestion di file .nii\n","from skimage.metrics import mean_squared_error as mse\n","from skimage.metrics import structural_similarity as ssim\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from skimage.metrics import normalized_root_mse as nrmse\n","from google.colab import drive\n","drive.mount('/content/drive/') #Ora necessario caricare su drive le cartelle, necessario però un metodo alternativo (caricare i file dal locale) COME?"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jZVx90c7FYC2","executionInfo":{"status":"ok","timestamp":1684226651661,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mario Di Giura","userId":"01764325961354702111"}}},"outputs":[],"source":["data_dir = '/content/drive/MyDrive/Data (prova)'  # directory della cartella \"data\" (già importata su drive)\n","\n","# Creazione di una lista di patients_ID\n","patients_ID = [] \n","for f in os.scandir(data_dir): # restituisce un iteratore sugli elementi presenti nella directory \"data_dir\"\n","  if f.is_dir(): # se l'elemento è una directory, allora viene aggiungo alla lista\n","    patients_ID.append(f.name) # riempimento della lista patients_ID con tutti i nomi delle directory che sono contenuti in data e soddisfano l'if"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"U9Hdr7u7kzvQ","executionInfo":{"status":"ok","timestamp":1684226652069,"user_tz":-120,"elapsed":410,"user":{"displayName":"Mario Di Giura","userId":"01764325961354702111"}}},"outputs":[],"source":["#data = '/content/drive/MyDrive/Data (prova)'  # directory\n","#patients_ID = []\n","#for f in os.scandir(data):\n","#  if f.is_dir():\n","#    patients_ID.append(f.name) # riempimento della lista patients_ID con tutti i nomi delle directory che sono contenuti in data e soddisfano l'if"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_NHN5UwFeFu"},"outputs":[],"source":["# Creazione di una lista patients_list\n","patients_list = []\n","for current in patients_ID:\n","  subj_dir = data_dir +'/'+ current \n","  \n","  #Creazione di un'istanza\n","  patient = tio.Subject( # creo un oggetto \"Subject\" per ogni paziente della lista utilizzando le immagini DSC e rCBV\n","      DSC = tio.ScalarImage(subj_dir + '/DSC.nii'), #.to(torch.float32), # ScalarImagine: immagine scalare 3D\n","      rCBV = tio.ScalarImage(subj_dir + '/DSC_ap-rCBV.nii'),  # ogni paziente contiene nome, immagine DSC e immagine rCBV\n","      brain_mask = tio.LabelMap(subj_dir + '/brain_mask.nii')\n","      )\n","  \n","  patient.DSC.set_data(patient.DSC.data.to(torch.float32))\n","  patient.rCBV.set_data(patient.rCBV.data.to(torch.float32))\n"," \n","  patients_list.append(patient)    # aggiunta di patient alla lista patients_list\n"]},{"cell_type":"markdown","metadata":{"id":"XLLbj8QCFwOx"},"source":["L'oggetto \"patient\" rappresenta un paziente specifico, contenente il nome del paziente e le immagini DSC e rCBV del paziente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"shw-M_Islwmk"},"outputs":[],"source":["#Creazione Dataset\n","dataset = tio.SubjectsDataset(patients_list)\n","print('Dataset size:', len(dataset), 'patients')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-jjFl2KzDN9"},"outputs":[],"source":["print(dataset[0][\"DSC\"][tio.DATA].dtype),\n","print(dataset[0][\"rCBV\"][tio.DATA].dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nn89XvJ3F1AV"},"outputs":[],"source":["!sudo apt-get install tree\n","#!tree -d /content/drive/MyDrive/Data # comando per visualizzare la struttura del dataset (usa !tree per visualizzare anche le sotto-directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWrXruZBF7Uv"},"outputs":[],"source":["# Funzione utile per visualizzare una rappresentazione testuale del dataset (utile per verifca)\n","# print(dataset.dry_iter())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KW_hB2ze-y7J"},"outputs":[],"source":["#for i in range(len(dataset)):\n","#  print(f\"paziente {i}:\")\n","#  subject = dataset[i]\n","#  for key, image in subject.items():\n","#    tensor = image.data\n","#    print(f\"  {key}: {tensor.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3C6IxbuBh6j"},"outputs":[],"source":["#creazione mask\n","#  rcbv_img=nib.load(subj_dir + \"/DSC_ap-rCBV.nii\")\n","  # converte l'immagine rCBV in un tensore PyTorch\n","#  rcbv_tensor = torch.from_numpy(rcbv_img.get_fdata())\n","#  # crea la brain_Mask selezionando i valori maggiori di 0\n","#  brain_mask = torch.where(rcbv_tensor > 0, torch.tensor([1]), torch.tensor([0]))\n","#  # crea un oggetto Nifti1Image a partire dal tensore PyTorch della brain_mask\n","#  brain_mask_nifti = nib.Nifti1Image(brain_mask.numpy(), rcbv_img.affine)\n","#  # salva l'immagine della brain_mask come file NIfTI-1 nella cartella\n","#  nib.save(brain_mask_nifti, subj_dir + '/brain_mask.nii')\n","\n","\n","#brain_mask = tio.LabelMap(subj_dir + '/brain_mask.nii')\n","#patient_x.brain_mask.plot()\n","#tio.ZNormalization(masking_method='brain_mask'),"]},{"cell_type":"markdown","metadata":{"id":"xx4QDhJZGBGi"},"source":["### Visualizzare tipo e immagine input output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bq1T3rkrGE0H"},"outputs":[],"source":["#dataset[1][\"DSC\"].plot()\n","#dataset[1][\"rCBV\"].plot()\n","# dataset[1].plot() #mostra direttamente entrambe le immagini"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wR4r0-bATX5V"},"outputs":[],"source":["# verifica su un paziente delle proprietà delle immagini\n","#patient_x = dataset[1] # numero casuale \n","#print('Shape of DSC images', patient_x.DSC.shape)\n","#print('Size of DSC images', patient_x.DSC.spacing)\n","#print('Shape of rCBV images', patient_x.rCBV.shape)\n","#print('Size of rCBV images', patient_x.rCBV.spacing)\n","#print('Size of brain mask', patient_x.brain_mask)\n","#patient_x.brain_mask.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5qsj3ifN3Yo"},"outputs":[],"source":["#Verifica lo shape delle immagini\n","#print('Shape of DSC images', training_set[0][\"DSC\"][tio.DATA].shape)\n","#print('Shape of rCBV images', training_set[0][\"DSC\"][tio.DATA].shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsN4USgwjb8u"},"outputs":[],"source":["#visualizzare un dataset\n","#paziente_1 = dataset[0]\n","#paziente_1.plot() #vedo sia DSC che rCBV\n","#patient.DSC.plot() #vedo solo DSC plot\n","#patient.rCBV.plot() #vedo solo rCBV plot"]},{"cell_type":"markdown","metadata":{"id":"BRkAOWASGRSQ"},"source":["## Training e Validation set"]},{"cell_type":"markdown","metadata":{"id":"U4K2dP3P8Zsr"},"source":["### Preparazione dei dati (trasformazioni)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRBm2Zsi8hnx"},"outputs":[],"source":["training_transform = tio.Compose([\n","    tio.ToCanonical(),\n","    tio.CropOrPad((120,120,80)), #Crop the images w,h,d and centre around the mask_name\n","    tio.Resample((2,2,2)),\n","    #tio.ZNormalization(masking_method='brain_mask'),\n","    tio.OneHot(),\n","])\n","\n","validation_transform = tio.Compose([\n","    tio.ToCanonical(),\n","    tio.CropOrPad((120,120,80)), #Crop the images w,h,d and centre around the mask_nam\n","    tio.Resample((2,2,2)),\n","    #tio.ZNormalization(masking_method='brain_mask'),\n","    tio.OneHot(),\n","])\n"]},{"cell_type":"markdown","metadata":{"id":"8G8qx8KXRDTe"},"source":["### Divisione del dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1smDIbO1GkAO"},"outputs":[],"source":["# suddivisione del dataset, in modo casuale, in dati di training (70% dei dati) e di validation (30% dei dati). #MANCA 10% PER IL TESTING!!!\n","training_division = 0.7   # definizione della % di dati di training\n","validation_division = 0.2 # definizione della % di dati di validation\n","#test_division = 0.1\n","\n","num_patients = len(dataset) # pazienti totali\n","train_patients = int(training_division*num_patients)\n","val_patients = int(validation_division*num_patients)\n","test_patients = int(num_patients-(train_patients+val_patients))\n","\n","# creazione dei tre sottoinsiemi \n","patients_division = train_patients, val_patients, test_patients\n","train, val, test = torch.utils.data.random_split(patients_list, patients_division) # suddivido casualmente la lista di pazienti in tre sottoinsiemi\n","\n","training_set = tio.SubjectsDataset(train, transform=training_transform)\n","print('train_set:', len(training_set)) # per verificare il numero di dati di training\n","validation_set = tio.SubjectsDataset(val, transform=validation_transform)\n","print('val_set:', len(validation_set)) # per verificare il numero di dati di validation\n","test_set = tio.SubjectsDataset(test)\n","print('test_set:', len(test_set))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1xBfibcJGuw"},"outputs":[],"source":["training_set[0][\"DSC\"][tio.DATA].shape, training_set[0][\"rCBV\"][tio.DATA].shape"]},{"cell_type":"markdown","metadata":{"id":"TuLR_rqiOb_6"},"source":["###Mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8rQZ4_9zjyv"},"outputs":[],"source":["#def masking(dataset):\n","#  for i in range(len(dataset)):\n","#    mask = dataset[i][\"DSC\"][tio.DATA] > 0\n","#   print(mask.shape)\n","#    dataset[i][\"mask\"] = mask "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hX8hjKr44WYF"},"outputs":[],"source":["#masking(dataset)\n","\n","#dataset[0][\"mask\"].plot()\n","#dataset[1][\"mask\"].plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_uJYRAVuqYLU"},"outputs":[],"source":[" #visualizzare info\n","#print(paziente_1.DSC.shape)\n","#mask = dataset[0][\"DSC\"][tio.DATA]>0\n","#print(mask.shape)\n","\n","#print(paziente_1.DSC)\n","#print(paziente_1.rCBV)"]},{"cell_type":"markdown","metadata":{"id":"ng0gpAbPt6FK"},"source":["##Creazione del DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSuhIsttyifE"},"outputs":[],"source":["#check num_cores disponibili\n","import multiprocessing\n","cores = multiprocessing.cpu_count() # per contare il numero di cpu che si hanno\n","cores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZkWonT8t9dS"},"outputs":[],"source":["from torch.utils.data.dataloader import DataLoader\n","batch_size = 8 # dimensione del batch\n","train_loader = torch.utils.data.DataLoader(training_set, \n","                                           batch_size, \n","                                           shuffle = True,\n","                                           num_workers=cores,\n","                                           pin_memory=True) # shuffle = True indica che i dati vengono caricati in modo casuale\n","val_loader = torch.utils.data.DataLoader(validation_set, \n","                                         batch_size, \n","                                         shuffle = False,\n","                                         num_workers=cores,  #metto tanti cores quanti ne ho disponibili\n","                                         pin_memory=True)  # i dati vengono caricati in ordine"]},{"cell_type":"markdown","metadata":{"id":"WGIav5_vj3qJ"},"source":["### Sposto il DataLoader e i dati sulla GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eW-_MIf3k--d"},"outputs":[],"source":["# Verifico che la GPU sia disponibile\n","torch.cuda.is_available() # Resistuisce 'True' se la GPU è accessibile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Sdh9mo4wZN_"},"outputs":[],"source":["def get_default_device():\n","    # Se è disponibile prende la GPU, altrimenti la CPU locale\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","    \n","#def to_device(data, device):\n","#    # Sposta i tensori (dati) sul device stabilito\n","#        return data.to(device, non_blocking=True)\n","#    if isinstance(data, torch.Tensor): # sposto i tensori\n","#    elif isinstance(data, (list,tuple)): # sposto le liste o le tuple\n","#        return [to_device(x, device) for x in data]\n","#    elif isinstance(data, dict): # sposto i dizionari\n","#        return {k: to_device(v, device) for k, v in data.items()}\n","#    else:\n","#        return data\n","\n","def to_device(data, device):\n","    if isinstance(data, (list,tuple)): \n","      return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    # Wrapping del DataLoader per spostare i dati sul device\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        # Restituisce un batch di dati spostandolo sulla GPU\n","        for b in self.dl: \n","            yield to_device(b, self.device) if isinstance(b, torch.Tensor) else b\n","\n","    def __len__(self):\n","        # Numero di batch\n","        return len(self.dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8l6Ftl_tTGY"},"outputs":[],"source":["device = get_default_device()\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5ENbW4MlXFu"},"outputs":[],"source":["train_loader = DeviceDataLoader(train_loader, device)  # crea un DeviceDataLoader \"avvolgendo\" (wrapping) il train_loader\n","val_loader = DeviceDataLoader(val_loader, device) # idem per il val_loader"]},{"cell_type":"markdown","metadata":{"id":"9Oz2vxg1wcQ7"},"source":["## Creazionde del modello"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0VoJMz2Jh3cU"},"outputs":[],"source":["#def set_parameter_requires_grad(model, feature_extracting):\n","#    if feature_extracting:\n","#        for param in model.parameters():\n","#            param.requires_grad = False\n","#permette di congelare tutti i pesi, non vengono più \"aggiustati\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RUyAu2cio4E"},"outputs":[],"source":["#resnet18 = torchvision.models.resnet18(pretrained=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xF-PoZc0lXNY"},"outputs":[],"source":["#print(resnet18)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5MtCSBHmIo-"},"outputs":[],"source":["#for param in resnet18.parameters():\n","#    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLQV5Ukoh6-M"},"outputs":[],"source":["#inizializzo e rimodello le reti dei modelli preaddestrati\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vafFnTMc8p5r"},"outputs":[],"source":["#type(training_set[0].DSC), \n","#training_set[0][\"rCBV\"].data #1: dice il tipo di dato che si ha #2: printa il contenuto del tensore"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tzAXWp-Jjv_"},"outputs":[],"source":["#def show_batch_DSC(data_loader):\n","#  for batch_idx, batch_sample in enumerate(data_loader):\n","#    images = batch_sample[\"DSC\"][tio.DATA]\n","#    img_DSC = images[0, 0, :, :, 50] #prendo solo la prima immagine, le x e le y della 50 fetta sulle z del time frame 0.\n","\n","#    # Visualizzazione delle prime 4 immagini della batch\n","#    plt.imshow(img_DSC, cmap=\"gray\")\n","#    plt.show()    # Interrompi l'iterazione dopo la prima batch\n","#    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECpLwZo9EabR"},"outputs":[],"source":["#show_batch_DSC(train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpOmuh9YR2GJ"},"outputs":[],"source":["def prepare_batch(batch, device): # estrae l'input e l'output desiderati da una singola batch di dati del loader e li restituisce come tensori PyTorch\n","    inputs = batch[\"DSC\"][tio.DATA].to(device) # tensore \n","    targets = batch[\"rCBV\"][tio.DATA].to(device) # tensore\n","    return inputs, targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXpfO3KxZ0gU"},"outputs":[],"source":["from torchmetrics.functional import structural_similarity_index_measure "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaNy_jkGC5Gv"},"outputs":[],"source":["class ImageClassificationBase(nn.Module):\n","    def training_step(self, batch):\n","        inputs, targets = prepare_batch(batch, device)\n","        print(inputs.shape)\n","        print(targets.shape)\n","        out = self(inputs)  # Generate predictions\n","        print(out.shape)             \n","        loss = F.mse_loss(out, targets) # Calculate loss\n","        print(\"Fine train\")\n","        return loss\n","\n","    def validation_step(self, loader):\n","        inputs, targets = prepare_batch(loader, device)\n","        out = self(inputs)     # Generate predictions\n","        loss = F.mse_loss(out, targets)   # Calculate loss\n","\n","        targets = targets.cpu().detach().numpy().squeeze() #converto in numpy perchè skimage lavora su tensori di questo tipo\n","        out = out.cpu().detach().numpy().squeeze()\n","      \n","        print(targets.shape, out.shape)\n","\n","        mse_ = np.square(np.subtract(targets, out)).mean() # Calculate mean square error\n","        nrmse_ = nrmse(targets, out) # Calculate normalized root mean square error\n","        ssim_ = structural_similarity_index_measure(torch.from_numpy(targets).to(torch.float32), torch.from_numpy(out).to(torch.float32), data_range = out.max() - out.min()) # Calculate structural similarity index\n","        psnr_ = psnr(targets, out, data_range = out.max() - out.min()) # Calculate peak signal to noise ratio\n","        return {'val_loss': loss.detach(), 'mse': mse_, 'nrmse': nrmse_, 'ssim': ssim_, 'psnr':psnr_}\n","   \n","    def validation_epoch_end(self, outputs):\n","      batch_losses = [x['val_loss'] for x in outputs]\n","      epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n","      batch_mses = [x['mse'] for x in outputs]\n","      epoch_mse = np.mean(batch_mses)   # Combine mses\n","      batch_nrmses = [x['nrmse'] for x in outputs]\n","      epoch_nrmse = np.mean(batch_nrmses)  # Combine nrmses\n","      batch_ssims = [x['ssim'] for x in outputs]\n","      epoch_ssim = np.mean(batch_ssims)   # Combine ssims\n","      batch_psnrs = [x['psnr'] for x in outputs]\n","      epoch_psnr = np.mean(batch_psnrs) # Combine psnrs\n","      return {'val_loss': epoch_loss.item(), 'mse': epoch_mse.item(), 'nrmse': epoch_nrmse.item(), 'ssim': epoch_ssim.item(), 'psnr':epoch_psnr.item()}\n","    \n","    def epoch_end(self, epoch, result):\n","      print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, mse: {:.4f}, nrmse: {:.4f}, ssim: {:.4f}, psnr: {:.4f}\".format(epoch+1, result['train_loss'], result['val_loss'], result['mse'], result['nrmse'], result['ssim'], result['psnr']))\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NM9HHDPKR1XJ"},"outputs":[],"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qE0-9g_pSenh"},"outputs":[],"source":["feature_extract = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UEk5mZLDBKH"},"outputs":[],"source":["import torch.nn as nn\n","import torchvision.models as models\n","\n","class CustomResNet(ImageClassificationBase):\n","    def __init__(self, num_classes, time_frame):\n","        super(CustomResNet, self).__init__()\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.encoder = models.video.r3d_18(pretrained=True)\n","        #set_parameter_requires_grad(self.encoder, feature_extract)\n","        self.encoder.stem[0] = nn.Conv3d(45, 64, kernel_size=(3, 3, 3), padding=1, bias=False)\n","        self.decoder = nn.Sequential(\n","    nn.ConvTranspose3d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1),\n","    nn.ConvTranspose3d(in_channels=128, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",")\n","\n","    def forward(self, x):\n","        x = self.encoder.stem(x)\n","        x = self.encoder.layer1(x)\n","        x = self.encoder.layer2(x)\n","        x = self.encoder.layer3(x)\n","        x = self.encoder.layer4(x)\n","        x = self.decoder(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGqHgVzCDPYM"},"outputs":[],"source":["model = CustomResNet(1,45)#.to('cuda')\n","model.encoder.fc = nn.Sequential() #per eliminare lo strato linere fc\n","model.encoder.avgpool = nn.Sequential()\n","model.encoder.layer4 = nn.Sequential()\n","# sposto il modello sulla GPU\n","model = to_device(model, device)\n","#print(model) \n","#print('Modello su dispositivo:', next(model.parameters()).device)"]},{"cell_type":"markdown","metadata":{"id":"vVSARCnGG-1o"},"source":["## Training del modello"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5u_JWh9R6Ano"},"outputs":[],"source":["# definisco una classe per salvare il modello durante il training. \n","# se il valore della val_loss corrente è minore del precedente, allora salvo lo stato del modello\n","\n","class SaveBestModel: \n","  def __init__(\n","      self, best_valid_loss = float('inf')\n","  ):\n","      self.best_valid_loss = best_valid_loss\n","\n","  def __call__(\n","      self, current_valid_loss,\n","      epoch, model, optimizer, criterion, path_name #, scheduler\n","  ): \n","      if current_valid_loss < self.best_valid_loss: \n","        self.best_valid_loss = current_valid_loss\n","        print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n","        print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n","        torch.save({\n","            'epoch': epoch+1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': criterion,\n","            }, path_name+'.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2kHrRdcmkAY"},"outputs":[],"source":["@torch.no_grad() #l'utilizzo di @torch.no_grad() all'interno della funzione evaluate() ha senso perché consente di disabilitare il calcolo del gradiente durante la fase di valutazione del modello, migliorando l'efficienza del processo di valutazione.\n","\n","def evaluate(model, val_loader):\n","  # with torch.no_grad():\n","    model.eval()\n","    print(\"Evaluate 1 fatto\")\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    print(\"Evaluate 2 fatto\")\n","    return model.validation_epoch_end(outputs)\n","\n","#def get_lr(optimizer):\n","#    for param_group in optimizer.param_groups:\n","#        return param_group['lr']\n","\n","#def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n","#                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n","#    torch.cuda.empty_cache()\n","#    history = []\n","#    \n","#    # Set up cutom optimizer with weight decay\n","#    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n","#    # Set up one-cycle learning rate scheduler\n","#    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n","#                                                steps_per_epoch=len(train_loader))\n","#    \n","#    for epoch in range(epochs):\n","#        \n","#        #strt_time = time.time()\n","#        \n","#        # Training Phase \n","#        model.train()\n","#        train_losses = []\n","#        lrs = []\n","#        for batch in train_loader:\n","#\n","#            \n","#            loss = model.training_step(batch)\n","#            loss.to(torch.float32)\n","#            train_losses.append(loss)\n","#            loss.backward()\n","#            \n","#            # Gradient clipping\n","#            if grad_clip: \n","#                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n","#            \n","#            optimizer.step()\n","#            optimizer.zero_grad()\n","#            \n","#            # Record & update learning rate\n","#            lrs.append(get_lr(optimizer))\n","#            scheduler.step()\n","#            \n","#        #end_time = time.time()\n","#        #tot_time = end_time-strt_time\n","#        #print('Epoch training time:', tot_time, 'sec')\n","#            \n","#        # Validation phase \n","#        result = evaluate(model, val_loader)\n","#        result['train_loss'] = torch.stack(train_losses).mean().item()\n","#        result['lrs'] = lrs\n","#        model.epoch_end(epoch, result)\n","#        history.append(result)\n","#        \n","#        #save_best_model(\n","#        #result['val_loss'], epoch, model, optimizer, scheduler, criterion, path_name\n","#    #)\n","#    return history\n","\n","def fit(epochs, lr, model, train_loader, val_loader, opt_func, path_name):\n","    torch.cuda.empty_cache()\n","    history = []\n","    optimizer = opt_func(model.parameters(), lr, weight_decay=0.1)\n","    for epoch in range(epochs):\n","\n","        # Training Phase \n","        print(\"Inizio del train\")\n","        model.train()\n","        train_losses = []\n","        for batch_idx, batch in enumerate(train_loader):\n","            loss = model.training_step(batch).detach().requires_grad_(True) #uso detach per provare a creare un nuovo tensore che possa fare il gradient descend\n","            train_losses.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # Validation phase\n","        print(\"Inizio fase di validazione\")\n","        result = evaluate(model, val_loader)\n","        print(\"Evaluate fatto\")\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        print(\"2 fatto\")\n","        model.epoch_end(epoch, result)\n","        print(\"3 fatto\")\n","        history.append(result)\n","\n","        save_best_model(\n","            result['val_loss'], epoch, model, optimizer, criterion, path_name\n","        )\n","\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsNnIkzaHKPH"},"outputs":[],"source":["num_epochs = 10\n","opt_func = torch.optim.Adam\n","lr=1e-2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDBMzQVoO6kS"},"outputs":[],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwKhOK2nSaNz"},"outputs":[],"source":["summary(model.cuda(), input_size=(2,45,60,60,40))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7gbthLgF115"},"outputs":[],"source":["evaluate(model, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccy-o5dTX8jB"},"outputs":[],"source":["#history1 = []\n","#history1 += fit_one_cycle(40, 0.01, model, train_loader, val_loader, weight_decay=0.01, grad_clip=20, opt_func=torch.optim.Adam)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hd0VBOGJAFKD"},"outputs":[],"source":["# loss function\n","criterion = nn.MSELoss()\n","\n","# optimizer \n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) #weight decay = 0.1\n","\n","# scheduler (none)\n","\n","# initialize SaveBestModel class\n","save_best_model = SaveBestModel()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKA0DKM7HJAt"},"outputs":[],"source":["history = fit(num_epochs, lr, model, train_loader, val_loader, opt_func, path_name='ResNet_best_model_train')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54Gm-uwEHLmx"},"outputs":[],"source":["def plot_val_loss(history):\n","    val_loss = [x['val_loss'] for x in history]\n","    plt.plot(val_loss, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('val_loss')\n","    plt.title('val_loss vs. No. of epochs')\n","    plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3QalRXNFoSl"},"outputs":[],"source":["def plot_train_loss(history):\n","    train_loss = [x['train_loss'] for x in history]\n","    plt.plot(train_loss, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('train_loss')\n","    plt.title('train_loss vs. No. of epochs')\n","    plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8y1orIxFKcD"},"outputs":[],"source":["def plot_MSE(history):\n","    mse = [x['mse'] for x in history]\n","    plt.plot(mse, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('MSE')\n","    plt.title('MSE vs. No. of epochs')\n","    plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkSKTPdgF6Qh"},"outputs":[],"source":["def plot_NRMSE(history):\n","    nrmse = [x['nrmse'] for x in history]\n","    plt.plot(nrmse, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('NRMSE')\n","    plt.title('NRMSE vs. No. of epochs')\n","    plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hPq1fJjjGGgI"},"outputs":[],"source":["def plot_PSNR(history):\n","    psnr = [x['psnr'] for x in history]\n","    plt.plot(psnr, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('PSNR')\n","    plt.title('PSNR vs. No. of epochs')\n","    plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKdmCuYFHPee"},"outputs":[],"source":["plot_val_loss(history), plot_train_loss(history), plot_MSE(history), plot_NRMSE(history), plot_PSNR(history)"]},{"cell_type":"markdown","metadata":{"id":"ZScEdUMKHR5o"},"source":["## Testing del modello"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ityHPnU_HQ5t"},"outputs":[],"source":["def predict_image(img, model):\n","    # Convert to a batch of 1\n","    xb = img  #.unsqueeze(0)\n","    # Get predictions from model\n","    yb = model(xb)\n","    #"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27yRQldFy_WP"},"outputs":[],"source":["#visualizzare un'immagine specifica\n","training_set[0].rCBV\n","#nii_array = nii_image.numpy() #bisogna trasformare l'immagin ein vettore con il comando numpy()\n","#plt.imshow(nii_array[1, :, :, 50], cmap='gray') #time_frame,x,y,z\n","#plt.show()\n","#training_set[0].DSC, training_set[0].rCBV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXjv07ydz_P_"},"outputs":[],"source":["dataset[0][\"DSC\"].plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fR1AdFFJ1VN_"},"outputs":[],"source":["xb = dataset[0][\"rCBV\"][tio.DATA]\n","print(xb.shape)\n","plt.imshow(xb[0, ...,90])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7o5oMay8gWYi"},"outputs":[],"source":["import random\n","subject = random.choice(validation_set)\n","\n","input_tensor = subject['DSC'][tio.DATA].to(device)\n","input_tensor.shape\n","\n","model.eval()\n","\n","with torch.no_grad():\n","    pred = model(input_tensor.unsqueeze(0).to(torch.float32)) \n","    plt.imshow(pred[0,0, ..., 30].cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUD7Xbtro7v4"},"outputs":[],"source":["pred.shape\n","input_tensor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V43QLE5qPTOO"},"outputs":[],"source":["print(dataset[0][\"DSC\"][tio.DATA].shape)\n","yb=dataset[0][\"DSC\"][tio.DATA].unsqueeze(dim=0)\n","print(yb.shape)\n","yb=model(dataset[0][\"DSC\"][tio.DATA].unsqueeze(dim=0).cuda().to(torch.float32))\n","plt.imshow(yb.detach().numpy()[0,0, :, :, 50], cmap='gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFezmUVs1WYe"},"outputs":[],"source":["torch.cuda.empty_cache()\n","device = torch.device(\"cpu\")\n","input_tensor = dataset[0][\"DSC\"][tio.DATA].cpu().to(torch.float32).unsqueeze(dim=0)#.to(model.device).to(torch.float32)\n","yb = model(input_tensor)\n","plt.imshow(yb.detach().cpu().numpy()[0, 0, :, :, 80], cmap='gray')"]},{"cell_type":"markdown","metadata":{"id":"ACeAVPtt-T8G"},"source":["Funziona, èquella presentata al secondo round. Le metriche ottenute non sono buone. Va migliorato il modello."]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1ohN6Dj359X8eXsm1yBSFbUBVS8XJytgP","timestamp":1683462727208}],"authorship_tag":"ABX9TyO9BQB3B5g5jKpD+BupQisV"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}